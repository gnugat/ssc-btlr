# cht:message

Using the `--help` option, the command will print the recommended way to run it:

```
./btlr cht:message --help
```

Copy paste the last line, and you're ready to go!

## Options

While it is recommened to use the example values given in the `--help` display,
it is possible to customize the different options.

### Manual Mode

When `--manual-mode true` is used, API calls to LLMs are disabled,
and the user is asked to:

* first copy paste the displayed prompts to their favorite LLMs (eg ChatGPT)
* then copy paste the LLM's completion back in the terminal

When given any other value (eg `--manual-mode false`, `--manual-mode 0`, etc),
then API calls to the configured, supported LLM are made.

> **Warning**: Currently no LLM API calls have been implemented,
> so only the Manual Mode will work.

### Config: LLM engine

When `--config-llm-engine "chatgpt-gpt-3.5-turbo"` is used,
[OpenAI's ChatGPT 3.5](https://chat.openai.com/) is set as the LLM engine to use.

> **Warning**: Currently no LLM API calls have been implemented,
> so this value is only used informatively in the logs to help determine how
> messages were generated.

### Config: Last Messages Filename

To enable Infinite Memory, and to be able to determine how each messages were
generated, the following need to be persisted somewhere as log entries:

* user prompt: what the user provided
* augmented prompt: the final prompt to be given to the LLM
* model completion: what the LLM generated

When `--config-logs-filename ./var/cht/logs` is used,
they are persisted locally on the filesystem at the given path.

The following filename template is used to save the log entries:

* `%logs_filename%/%directory%/%time%_%priority%_%id%_%type%.json`

The placeholders will then be replaced with the appropriate values:

* `logs_filename`: with the value from the option
  (eg `./var/cht/logs`)
* `%time%`: a time generated by the system
* `%id%`: a UUID generated by the system
* `%type%`, and its associated `%priority%` and `%directory%`: one of
  * `user_prompt`, its priority is `000`, its directory `last_messages`
  * `augmented_prompt`, its priority is `500`, its directory `last_messages`
  * `model_completion`, its priority is `900`, its directory `last_messages`

The log entries are stored in chronological order to make it easier to read
(the priority bit is used to make sure that log entries are listed in logical
order, for example that the user prompt always appear before the model
completion)

### Config: Prompt Templates

The user prompt is augmented by using prompt templates.

Custom ones can be used with the following options:

* `--config-prompt-templates-filename`

### Config: Chunk Memory Size

The value of `--chunk-memory-size` needs to be a multiple of 2,
as for each user prompt there are currently 2 log files in messages:
one for the user prompt and one for the model completion.

There is a Memory Pointer that will initially point to the first ever log.

If `--chunk-memory-size 10` is used, whenever the number of logs after the
memory pointer exceeds the configured Chunk Memory Size, a summary of the 
10 logs is generated, and the Memory Pointer gets moved to the 11th message.

The Last Messages section in the augmented prompt only includes logs from
the Memory Pointer on.

The Memory Extract section in the augmented prompt includes all the generated
log summaries.

## Further documentation

You can find more information in the
`./doc/02-cht/01-message/01-prompt-templates.md` file.
