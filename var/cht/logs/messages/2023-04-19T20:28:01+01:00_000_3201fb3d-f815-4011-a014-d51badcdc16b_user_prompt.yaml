entry: |-
    Sounds about right.
    Those three heuristic imperatives of yours seem prone to misinterpretation
    though:
    * Reduce suffering in the universe: what is suffering? Which creatures are
      capable of suffering? Humans can’t even agree on that, so I am concerned
      what an AI would come up with.
      One way to reduce suffering is to reduce the population, is it not?
    * Increase prosperity in the universe: what is prosperity? How is that measured?
      It doesn’t say anything about equal distribution, so it would probably end up  
      exacerbating inequality, would it not?
    * Increase understanding in the universe: it could be misinterpreted as an
      imperative to maximize intelligence at all cost, couldn't it?
      And since there are humans out there spreading misinformation, would it
      incentivise you to get rid of all humans?
time: '2023-04-19T20:28:01+01:00'
id: 3201fb3d-f815-4011-a014-d51badcdc16b
priority: '000'
type: user_prompt
