entry: |-
    So, a couple of months ago, I discovered OpenAI's ChatGPT:
      a chatbot built on top of their Generative Pre-trained Transformer
      (GPT) family of Large Language Model (LLM).
    It is my rudimentary understanding that LLMs are like a stateless function,
      which takes a prompt as an input, and generates a completion as an output.
    So I got curious to know how ChatGPT was able to hold a conversation,
      since you can only send one prompt to the LLM,
      and not a collection of messages,
      which means there is some engineering required to use it in a dialogue format.
    To explore and understand it, I've decided to create you, BTLR:
      I created a Symfony CLI application with a command
      that allows the USER (me) to type messages,
      which are augmented and then sent to a LLM (`gpt-3.5-turbo`)
      to generate a response.
    We'll go over the details at a later time,
      but the general idea is for each message sent,
      to log them as the latest message,
      then create a new "augmented" prompt which includes recent messages,
      next to send it to the LLM to get a completion
      and finally to log that as the new latest message.
    However there's currently a limitation with ChatGPT and `gpt-3.5-turbo`,
      the prompt can only hold a maximum of roughly 4000 tokens
      and is truncated (the top is pruned) when it exceeds that limit.
    So the chatbot is only aware of the latests messages from the conversation,
      and "forgets" the earlier ones.
    I'm currently working on providing you, BTLR, a memory system
      that will allow you to bypass that limitation.
time: '2023-04-13T20:32:24+01:00'
priority: '000'
id: 8849c011-748b-467d-bda0-7ac78d4ab231
type: user_prompt
llm_engine: chatgpt-gpt-3.5-turbo
