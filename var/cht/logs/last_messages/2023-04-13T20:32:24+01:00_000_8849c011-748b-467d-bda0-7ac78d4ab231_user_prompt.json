{"entry":"So, a couple of months ago, I discovered OpenAI's ChatGPT:\n  a chatbot built on top of their Generative Pre-trained Transformer\n  (GPT) family of Large Language Model (LLM).\nIt is my rudimentary understanding that LLMs are like a stateless function,\n  which takes a prompt as an input, and generates a completion as an output.\nSo I got curious to know how ChatGPT was able to hold a conversation,\n  since you can only send one prompt to the LLM,\n  and not a collection of messages,\n  which means there is some engineering required to use it in a dialogue format.\nTo explore and understand it, I've decided to create you, BTLR:\n  I created a Symfony CLI application with a command\n  that allows the USER (me) to type messages,\n  which are augmented and then sent to a LLM (`gpt-3.5-turbo`)\n  to generate a response.\nWe'll go over the details at a later time,\n  but the general idea is for each message sent,\n  to log them as the latest message,\n  then create a new \"augmented\" prompt which includes recent messages,\n  next to send it to the LLM to get a completion\n  and finally to log that as the new latest message.\nHowever there's currently a limitation with ChatGPT and `gpt-3.5-turbo`,\n  the prompt can only hold a maximum of roughly 4000 tokens\n  and is truncated (the top is pruned) when it exceeds that limit.\nSo the chatbot is only aware of the latests messages from the conversation,\n  and \"forgets\" the earlier ones.\nI'm currently working on providing you, BTLR, a memory system\n  that will allow you to bypass that limitation.","time":"2023-04-13T20:32:24+01:00","priority":"000","id":"8849c011-748b-467d-bda0-7ac78d4ab231","type":"user_prompt","llm_engine":"chatgpt-gpt-3.5-turbo"}
